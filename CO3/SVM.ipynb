{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4425cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6f14fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d082379f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> d\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> l\n",
      "Packages:\n",
      "  [ ] abc................. Australian Broadcasting Commission 2006\n",
      "  [ ] alpino.............. Alpino Dutch Treebank\n",
      "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
      "  [ ] basque_grammars..... Grammars for Basque\n",
      "  [ ] bcp47............... BCP-47 Language Tags\n",
      "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
      "                           Extraction Systems in Biology)\n",
      "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
      "  [ ] book_grammars....... Grammars from NLTK Book\n",
      "  [ ] brown............... Brown Corpus\n",
      "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
      "  [ ] cess_cat............ CESS-CAT Treebank\n",
      "  [ ] cess_esp............ CESS-ESP Treebank\n",
      "  [ ] chat80.............. Chat-80 Data Files\n",
      "  [ ] city_database....... City Database\n",
      "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
      "  [ ] comparative_sentences Comparative Sentence Dataset\n",
      "  [ ] comtrans............ ComTrans Corpus Sample\n",
      "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
      "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
      "Hit Enter to continue: \n",
      "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
      "                           and Basque Subset)\n",
      "  [ ] crubadan............ Crubadan Corpus\n",
      "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
      "  [ ] dolch............... Dolch Word List\n",
      "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
      "                           Corpus\n",
      "  [ ] extended_omw........ Extended Open Multilingual WordNet\n",
      "  [ ] floresta............ Portuguese Treebank\n",
      "  [ ] framenet_v15........ FrameNet 1.5\n",
      "  [ ] framenet_v17........ FrameNet 1.7\n",
      "  [ ] gazetteers.......... Gazeteer Lists\n",
      "  [ ] genesis............. Genesis Corpus\n",
      "  [ ] gutenberg........... Project Gutenberg Selections\n",
      "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
      "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
      "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
      "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
      "                           ChaSen format)\n",
      "  [ ] kimmo............... PC-KIMMO Data Files\n",
      "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
      "Hit Enter to continue: \n",
      "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
      "                           for parser comparison\n",
      "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
      "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
      "                           part-of-speech tags\n",
      "  [ ] machado............. Machado de Assis -- Obra Completa\n",
      "  [ ] masc_tagged......... MASC Tagged Corpus\n",
      "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
      "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
      "  [ ] moses_sample........ Moses Sample Models\n",
      "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
      "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
      "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
      "                           2015) subset of the Paraphrase Database.\n",
      "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
      "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
      "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
      "  [ ] nps_chat............ NPS Chat\n",
      "  [ ] omw-1.4............. Open Multilingual Wordnet\n",
      "  [ ] omw................. Open Multilingual Wordnet\n",
      "  [ ] opinion_lexicon..... Opinion Lexicon\n",
      "Hit Enter to continue: x\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> x\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> q\n"
     ]
    }
   ],
   "source": [
    "nltk.download_shell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "849c9c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        v1                                                 v2 Unnamed: 2  \\\n",
       "0      ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1      ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3      ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "...    ...                                                ...        ...   \n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...        NaN   \n",
       "5568   ham              Will Ì_ b going to esplanade fr home?        NaN   \n",
       "5569   ham  Pity, * was in mood for that. So...any other s...        NaN   \n",
       "5570   ham  The guy did some bitching but I acted like i'd...        NaN   \n",
       "5571   ham                         Rofl. Its true to its name        NaN   \n",
       "\n",
       "     Unnamed: 3 Unnamed: 4  \n",
       "0           NaN        NaN  \n",
       "1           NaN        NaN  \n",
       "2           NaN        NaN  \n",
       "3           NaN        NaN  \n",
       "4           NaN        NaN  \n",
       "...         ...        ...  \n",
       "5567        NaN        NaN  \n",
       "5568        NaN        NaN  \n",
       "5569        NaN        NaN  \n",
       "5570        NaN        NaN  \n",
       "5571        NaN        NaN  \n",
       "\n",
       "[5572 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dst=pd.read_csv('spam.csv',encoding=\"ISO-8859-1\",sep=',')\n",
    "dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "58eca656",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>messages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                           messages\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages=pd.read_csv('spam.csv', encoding = \"ISO-8859-1\",sep=',')\n",
    "messages=messages.iloc[:,0:2]\n",
    "messages.columns=['label','messages']\n",
    "\n",
    "# messages.rename(columns={'v1':'label','v2':'messages'},inplace=True)\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5efae8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "99825769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d049221d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b1b2e395",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dwany\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e105efdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_mesg(msg):\n",
    "    nopunc=[i for i in msg if i not in string.punctuation]\n",
    "    nopunc=\"\".join(nopunc)\n",
    "    return [i for i in  nopunc.split() if i.lower() not in stopwords.words(\"english\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "ec9b89f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Go, jurong, point, crazy, Available, bugis, n...\n",
       "1                       [Ok, lar, Joking, wif, u, oni]\n",
       "2    [Free, entry, 2, wkly, comp, win, FA, Cup, fin...\n",
       "3        [U, dun, say, early, hor, U, c, already, say]\n",
       "4    [Nah, dont, think, goes, usf, lives, around, t...\n",
       "Name: messages, dtype: object"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages['messages'].head(5).apply(clean_mesg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "5b99ea5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2f1cda39",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=CountVectorizer(analyzer=clean_mesg).fit(messages['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "2e9cbde7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11304\n"
     ]
    }
   ],
   "source": [
    "print(len(cv.vocabulary_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "367baf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs=messages['messages'][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "5dced919",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Even my brother is not like to speak with me. They treat me like aids patent.\n"
     ]
    }
   ],
   "source": [
    "print(msgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "690ed702",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvtran=cv.transform([msgs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "2f1c5a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1777)\t1\n",
      "  (0, 4512)\t1\n",
      "  (0, 5111)\t1\n",
      "  (0, 7698)\t2\n",
      "  (0, 8653)\t1\n",
      "  (0, 9860)\t1\n",
      "  (0, 10513)\t1\n"
     ]
    }
   ],
   "source": [
    "print(cvtran)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "0207f4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dwany\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'like'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names()[7698]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "93691c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesg_cv=cv.transform(messages['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "9a030920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Sparse Matrix: (5572, 11304)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of Sparse Matrix:',mesg_cv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "a9045083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50193"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mesg_cv.nnz #num  of non zero elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "69d1426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TERM FREQUENCY-INVERSE DOCUMENT FREQUENCY\n",
    "\n",
    "#difference between TF and DF\n",
    "#TF is frequency counter for a term t in document d\n",
    "#DF is the count of occurences of term t in the document set N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "71519f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "64c03d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 10513)\t0.33521441416639297\n",
      "  (0, 9860)\t0.32680848361658965\n",
      "  (0, 8653)\t0.43692878744892083\n",
      "  (0, 7698)\t0.4149098236609654\n",
      "  (0, 5111)\t0.3383712690711893\n",
      "  (0, 4512)\t0.43692878744892083\n",
      "  (0, 1777)\t0.33521441416639297\n"
     ]
    }
   ],
   "source": [
    "tfidf_trans=TfidfTransformer().fit(mesg_cv)\n",
    "tfidf=tfidf_trans.transform(cvtran)\n",
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "9c9de10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating data pipeline svm\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "20ba448b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "m_train,m_test,l_train,l_test=train_test_split(messages['messages'],messages['label'],test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "738435fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline \n",
    "pline=Pipeline([\n",
    "    ('a',CountVectorizer(analyzer=clean_mesg)),\n",
    "    ('b',TfidfTransformer()),\n",
    "    ('c',svm.SVC(C=0.1,kernel='linear',degree=3,gamma='auto')),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "ee4e98c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('a',\n",
       "                 CountVectorizer(analyzer=<function clean_mesg at 0x000002044B3758B0>)),\n",
       "                ('b', TfidfTransformer()),\n",
       "                ('c', SVC(C=0.1, gamma='auto', kernel='linear'))])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pline.fit(m_train,l_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "026a8105",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=pline.predict(m_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "431b5c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       1.00      0.91      0.95      1077\n",
      "        spam       0.28      1.00      0.44        38\n",
      "\n",
      "    accuracy                           0.91      1115\n",
      "   macro avg       0.64      0.95      0.70      1115\n",
      "weighted avg       0.98      0.91      0.94      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predictions,l_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec52df74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
